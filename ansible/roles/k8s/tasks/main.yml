---

- name: Update /etc/hosts - Remove hostname to 127.0.0.1 resolved hosts
  lineinfile: dest=/etc/hosts regexp='^127\.0\.0\.1' line='127.0.0.1 localhost' owner=root group=root mode=0644
  become: yes

- name: Update /etc/hosts - Ensure private ip for master1 resolved hosts
  lineinfile: dest=/etc/hosts regexp='{{ k8s_master_hostname }}$' line='{{ k8s_master_ip }} {{ k8s_master_hostname }}' owner=root group=root mode=0644
  become: yes

- name: Update /etc/hosts - Ensure private ip for k8s nodes resolved hosts
  lineinfile: dest=/etc/hosts regexp='{{ item.name }}$' line='{{ item.ip }} {{ item.name }}' owner=root group=root mode=0644
  loop: "{{ k8s_nodes }}"
  become: yes

- name: Restart kubeproxy at reboot
  copy:
    content: |
             #!/bin/bash
             sudo -i -u {{ ansible_user}} nohup kubectl proxy --address={{ k8s_master_ip }} --accept-hosts='^.*$' &
             exit
    dest: /etc/rc.local
    owner: root
    mode: 0755
  become: yes
  when:
    - kubernetes_node_type == 'admin'
    - is_cloud == '0'

- name: swapoff
  shell: swapoff -a
  args:
    creates: /home/{{ ansible_user }}/.k8sinstall
  become: yes

- name: Remove Swap from fstab
  lineinfile:
    dest: /etc/fstab
    regexp: '^.*swap.*$'
    state: absent
  become: yes

- name: Ensure /etc/kubernetes folder is present
  file:
    path: /etc/kubernetes
    owner: root
    state: directory
    mode: 0755
  become: yes

- name: Install route package
  apt:
    name: net-tools
    state: present
    update_cache: true
    force: yes
  become: yes

- name: K8s ip routed by public master1
  shell: |
    /sbin/route -n | grep 10.96.0.1
    (($? == 0)) && exit 0
    route add 10.96.0.1 gw {{ k8s_master_ip }}
  args:
    executable: /bin/bash
  become: yes
  when:
    - kubernetes_node_type == 'worker'
    - is_cloud == '0'

- name: K8s ip route at start
  copy:
    content: |
             #!/bin/bash
             route add 10.96.0.1 gw {{ k8s_master_ip }}
    dest: /etc/network/if-up.d/routemaster
    owner: root
    mode: 0755
  become: yes
  when:
    - kubernetes_node_type == 'worker'
    - is_cloud == '0'

- name: generate cloud.conf
  template:
    src: templates/cloud.conf.j2
    dest: /etc/kubernetes/cloud.conf
    mode: 0644
  become: yes
  when: is_cloud == '1'

- name: generate kubeadm.conf
  template:
    src: templates/nocloud-kubeadm.conf.j2
    dest: /etc/kubernetes/kubeadm.conf
    mode: 0644
  become: yes
  when: is_cloud == '0'

- name: generate kubeadm.conf
  template:
    src: templates/cloud-kubeadm.conf.j2
    dest: /etc/kubernetes/kubeadm.conf
    mode: 0644
  become: yes
  when: is_cloud == '1'

- name: cloud ssl certificate file
  copy:
    src: "{{ cloud_crt_path }}"
    dest: /usr/local/share/ca-certificates/cloud.crt
  become: yes
  when:
    - is_cloud == '1'
    - cloud_crt_path != ''

- name: add cloud cert to certificates store
  shell: update-ca-certificates
  become: yes
  when:
    - is_cloud == '1'
    - cloud_crt_path != ''

- name: Resolv.conf on k8s dns pod
  lineinfile:
    dest: /etc/systemd/resolved.conf
    regexp: "{{ item.regex }}"
    line: "{{ item.line }}"
  loop:
    - regex: '^#DNS='
      line: 'DNS=10.96.0.10'
    - regex: '^#Domains='
      line: 'Domains=svc.cluster.local'
  become: yes

- name: Restart resolved service
  systemd:
    name: systemd-resolved
    state: restarted
  become: yes

- name: Kubelet cloud arguments
  lineinfile:
    dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
    regexp: "^Environment=\"KUBELET_KUBECONFIG_ARGS\".*$"
    line: "Environment=\"KUBELET_KUBECONFIG_ARGS=--cloud-provider=openstack --cloud-config=/etc/kubernetes/cloud.conf --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\""
  become: yes
  when: is_cloud == '1'

- name: Restart kubelet service
  systemd:
    name: kubelet
    state: restarted
    enabled: yes
    daemon_reload: yes
  become: yes

- name: Ensure kubelet is in a running state
  service:
    name: kubelet
    state: started
  register: kubelet_details
  until: kubelet_details.status.ActiveState == "active"
  retries: 15
  delay: 20
  become: yes

#- name: Ensure kubelet is running and ready
#  pause:
#    seconds: 90

- name: init master
  shell: |
    kubeadm reset -f
    kubeadm init --config /etc/kubernetes/kubeadm.conf
  args:
    creates: /home/{{ ansible_user }}/.k8sinstall
  run_once: true
  become: yes
  when:
    - kubernetes_node_type == 'admin'

- name: "init {{ ansible_user }} env"
  shell: >
    mkdir -p /home/{{ ansible_user }}/.kube;
    sudo cp -f /etc/kubernetes/admin.conf /home/{{ ansible_user }}/.kube/config;
    sudo chown {{ ansible_user }}:{{ ansible_user }} /home/{{ ansible_user }}/.kube/config;
  run_once: true
  when: kubernetes_node_type == 'admin'

- name: get token
  shell:  kubeadm token list | tail -n +2 | head -n 1 | sed "s; .*;;"
  register: master_token
  run_once: true
  become: yes
  when: kubernetes_node_type == 'admin'

- name: Weave CNI
  shell: kubectl apply -f "https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml"
  args:
    creates: ~/.k8sinstall
  run_once: true
  when:
    - kubernetes_node_type == 'admin'
    - k8s_cni == 'weave'

- name: generate flannel manifest
  template:
    src: templates/kube-flannel.yml.j2
    dest: kube-flannel.yml
    mode: 0644
  when:
    - kubernetes_node_type == 'admin'
    - k8s_cni == 'flannel'

- name: Flannel CNI
  shell: |
    kubectl apply -f kube-flannel.yml
  args:
    creates: ~/.k8sinstall
  run_once: true
  when:
    - kubernetes_node_type == 'admin'
    - k8s_cni == 'flannel'

- name: generate calico manifest
  template:
    src: templates/calico-custom-resources.yml.j2
    dest: calico-custom-resources.yml
    mode: 0644
  when:
    - kubernetes_node_type == 'admin'
    - k8s_cni == 'calico'

- name: Calico CNI
  shell: |
    kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml
    sleep 5
    kubectl apply -f calico-custom-resources.yml
  args:
    creates: ~/.k8sinstall
  run_once: true
  when:
    - kubernetes_node_type == 'admin'
    - k8s_cni == 'calico'

- name: grant dashboard permissions
  shell: kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard
  args:
    creates: ~/.k8sinstall
  run_once: true
  when: kubernetes_node_type == 'admin'

- name: Add metrics addon
  shell: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.1/components.yaml
  args:
    creates: ~/.k8sinstall
  run_once: true
  when: kubernetes_node_type == 'admin'

- name: Add k8snode
  shell: "kubeadm join --token {{ hostvars[groups['master'][0]]['master_token'].stdout }} --discovery-token-unsafe-skip-ca-verification --node-name {{ inventory_hostname }} --cri-socket=unix:///var/run/crio/crio.sock {{ k8s_master_ip }}:443"
  args:
    creates: /home/{{ ansible_user }}/.k8sinstall
  become: yes
  when:
    - kubernetes_node_type == 'worker'

- name: proxy master
  shell: nohup kubectl proxy --address={{ k8s_master_ip }} --accept-hosts='^.*$' &
  args:
    creates: ~/.k8sinstall
  when:
    - kubernetes_node_type == 'admin'
    - is_cloud == '0'

- name: Cinder storage class file
  template:
    src: templates/cinder-sc.yml.j2
    dest: /etc/kubernetes/cinder-sc.yml
    mode: 0644
  become: yes
  when:
    - kubernetes_node_type == 'admin'
    - is_cloud == '1'

- name: Apply storage class
  shell: kubectl apply -f /etc/kubernetes/cinder-sc.yml
  when:
    - kubernetes_node_type == 'admin'
    - is_cloud == '1'

- name: Resolv.conf on k8s dns pod
  copy:
    content: |
             nameserver 10.96.0.10
             nameserver 8.8.8.8
             search svc.cluster.local
    dest: /etc/resolv.conf
  become: yes

- name: get contents of authkey
  command: cat /home/{{ ansible_user }}/.ssh/authorized_keys
  register: authkey

- name: Set authorized key for root user
  authorized_key:
    user: "root"
    state: present
    key: "{{ authkey.stdout }}"
  become: yes

- name: end install
  file:
    dest: ~/.k8sinstall
    state: touch
    force: yes
